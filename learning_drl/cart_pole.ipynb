{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_layer = torch.nn.Linear(4,128)\n",
    "        self.hidden_layer_1 = torch.nn.Linear(128,128)\n",
    "        self.output_layer = torch.nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output = self.input_layer(x)\n",
    "        output = F.leaky_relu_(output)\n",
    "        output = self.hidden_layer_1(output)\n",
    "        output = F.leaky_relu_(output)\n",
    "        output = self.output_layer(output)\n",
    "        return output      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(Dataset):\n",
    "    \n",
    "    def __init__(self,batch_size:int=32,memory_len:int=100000, *args, **kwargs):\n",
    "        self.__memory = []\n",
    "        self.__memory_len = memory_len\n",
    "        self.__batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        memory_as_tensor = torch.tensor(self.__memory,device=device,dtype=torch.float32)\n",
    "        indexes = torch.randint(low=0,high=len(self.__memory),size=(min(self.__batch_size,len(self.__memory)),))\n",
    "        return memory_as_tensor[indexes]\n",
    "        \n",
    "    def push(self,env_output):\n",
    "        self.__memory.append([env_output])\n",
    "        if len(self.__memory) > self.__memory_len:\n",
    "            del self.__memory[0]\n",
    "    \n",
    "    def __bool__(self):\n",
    "        return len(self.__memory) > self.__batch_size    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_values:torch.Tensor):\n",
    "    if torch.rand(size=(1,)) <0.2:\n",
    "        return torch.randint(2,size=(1,)).cpu().item()\n",
    "    return torch.argmax(q_values.clone().detach().view(-1).cpu(),dim=0).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_action(q_values:torch.Tensor):\n",
    "    return torch.max(q_values.clone().detach(),dim=0).values.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "target_network = Model().to(device)\n",
    "policy_network = Model().to(device)\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "optimizer = torch.optim.AdamW(policy_network.parameters(),lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer=optimizer,end_factor=1e-8,total_iters=500)\n",
    "memory = Memory()\n",
    "target_network = target_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fc66723ab94030a4c36b44d04d2856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = 0\n",
    "for epoch in tqdm(range(1000)):\n",
    "    done = False\n",
    "    past_observation, info = env.reset()\n",
    "    ep_return = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = choose_action(policy_network(torch.tensor(past_observation,device=device,dtype=torch.float32)))\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = truncated or terminated\n",
    "            memory.push(env_output=[*observation,*past_observation,int(action), reward,terminated])\n",
    "            past_observation = observation\n",
    "        if memory:\n",
    "            for i, batch in enumerate(memory):\n",
    "                batch = batch.squeeze()\n",
    "                action_indexes = (F.one_hot(batch[:,8].view(-1,1).to(torch.int64),num_classes=2)==1).view(-1,2)\n",
    "                current_state_outputs = policy_network(batch[:,4:8])[action_indexes]\n",
    "                with torch.no_grad():\n",
    "                    next_state_outputs = target_network(batch[:,:4])\n",
    "                    next_state_outputs = torch.max(next_state_outputs,dim=1).values\n",
    "                    next_state_outputs = batch[:,9]+ ( (~batch[:,10].to(torch.bool))*0.99*next_state_outputs)\n",
    "                    next_state_outputs = (next_state_outputs +1) / 2\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(current_state_outputs.to(device).squeeze(),next_state_outputs.to(device).squeeze())\n",
    "                writer.add_scalar('loss',loss.cpu().item(),steps)\n",
    "                writer.add_scalar('reward',ep_return,steps)\n",
    "                steps += 1\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                break\n",
    "        ep_return += reward        \n",
    "    if (epoch+1)%10 == 0:\n",
    "        target_network.load_state_dict(policy_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musasina/anaconda3/envs/teknofest/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "past_observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_network(torch.tensor(past_observation,device=device,dtype=torch.float32))\n",
    "        action = torch.argmax(q_values,dim=0).cpu().item()  # agent policy that uses the observation and info\n",
    "        env_output = env.step(action)\n",
    "        observation, reward, terminated, truncated, info = env_output\n",
    "        if not(terminated or truncated):\n",
    "            past_observation = observation\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teknofest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
